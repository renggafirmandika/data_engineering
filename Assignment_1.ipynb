{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> <span style=\"color:white\">Electricity Sector Data Integration & Augmentation</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> <span style=\"color:white\">GROUP 04</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name                   | SID       | Unikey   |\n",
    "| ---------------------- | --------- | -------- |\n",
    "| Putu Eka Udiyani Putri | 550067302 | pput0940 |\n",
    "| Rengga Firmandika      | 550126632 | rfir0117 |\n",
    "| Vincentius Ansel Suppa | 550206406 | vsup0468 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">1. Data Acquisition</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">a. National Greenhouse and Energy Reporting (NGER)</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "BASE = \"https://api.cer.gov.au/datahub-public/v1\"\n",
    "SCHEME = \"NGER\"\n",
    "HEADERS = {\"Accept\": \"application/json\", \"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "OUT = Path(\"DATA1_ELECTRICITY\"); OUT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def get_json(path: str):\n",
    "    url = BASE + path\n",
    "    r = requests.get(url, headers=HEADERS, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total catalog items under NGER: 112\n",
      "\n",
      "Targets discovered:\n",
      "  2014-15  ID0075\n",
      "  2015-16  ID0076\n",
      "  2016-17  ID0077\n",
      "  2017-18  ID0078\n",
      "  2018-19  ID0079\n",
      "  2019-20  ID0080\n",
      "  2020-21  ID0081\n",
      "  2021-22  ID0082\n",
      "  2022-23  ID0083\n",
      "  2023-24  ID0243\n"
     ]
    }
   ],
   "source": [
    "# List schemes\n",
    "schemes = get_json(\"/api/Schemes\")\n",
    "nger = next((s for s in schemes if (s.get(\"id\") or \"\").upper() == SCHEME), None)\n",
    "if not nger:\n",
    "    raise SystemExit(\"Scheme NGER not found in /api/Schemes\")\n",
    "\n",
    "# Get all DatasetCatalogItems for NGER\n",
    "items = get_json(f\"/api/Schemes/{SCHEME}/DatasetCatalogItems\")\n",
    "print(f\"Total catalog items under {SCHEME}: {len(items)}\")\n",
    "\n",
    "# Filter only electricity sector datasets 2014–2024\n",
    "PHRASE = \"greenhouse and energy information by designated generation facility\"\n",
    "YEARS = [f\"{y}-{str(y+1)[-2:]}\" for y in range(2014, 2024)]\n",
    "YEARS_SET = set(YEARS)\n",
    "\n",
    "targets = []\n",
    "for it in items:\n",
    "    cid   = str(it.get(\"id\") or \"\")\n",
    "    title = str(it.get(\"displayName\") or \"\").lower()\n",
    "    if PHRASE in title:\n",
    "        m = re.search(r\"(20\\d{2}[–-]\\d{2})\", title)\n",
    "        fy = m.group(1).replace(\"–\",\"-\") if m else \"\"\n",
    "        if fy in YEARS_SET:\n",
    "            targets.append((fy, cid))\n",
    "\n",
    "# Sort chronologically\n",
    "def fy_key(fy: str):\n",
    "    a, b = fy.split(\"-\")\n",
    "    return (int(a), int(b))\n",
    "targets.sort(key=lambda t: fy_key(t[0]))\n",
    "\n",
    "print(\"\\nTargets discovered:\")\n",
    "for fy, cid in targets:\n",
    "    print(f\"  {fy}  {cid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2014-15 -> nger_2014-15.csv (48.0 KB)\n",
      "[OK] 2015-16 -> nger_2015-16.csv (48.5 KB)\n",
      "[OK] 2016-17 -> nger_2016-17.csv (52.0 KB)\n",
      "[OK] 2017-18 -> nger_2017-18.csv (55.2 KB)\n",
      "[OK] 2018-19 -> nger_2018-19.csv (63.1 KB)\n",
      "[OK] 2019-20 -> nger_2019-20.csv (66.6 KB)\n",
      "[OK] 2020-21 -> nger_2020-21.csv (69.9 KB)\n",
      "[OK] 2021-22 -> nger_2021-22.csv (73.2 KB)\n",
      "[OK] 2022-23 -> nger_2022-23.csv (73.7 KB)\n",
      "[OK] 2023-24 -> nger_2023-24.csv (81.4 KB)\n",
      "\n",
      "Done. Downloaded 10/10 files into /Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA1_ELECTRICITY\n"
     ]
    }
   ],
   "source": [
    "# Download raw files\n",
    "downloaded = 0\n",
    "for fy, cid in targets:\n",
    "    ok = False\n",
    "    for ext in [\"csv\", \"xlsx\"]:\n",
    "        url = f\"{BASE}/api/Dataset/{SCHEME}/dataset/{cid}.{ext}\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=120)\n",
    "            if r.ok and len(r.content) > 500:  # simple sanity check\n",
    "                out = OUT / f\"nger_{fy}.{ext}\"\n",
    "                out.write_bytes(r.content)\n",
    "                print(f\"[OK] {fy} -> {out.name} ({len(r.content)/1024:.1f} KB)\")\n",
    "                downloaded += 1\n",
    "                ok = True\n",
    "                break\n",
    "            else:\n",
    "                print(f\"[TRY] {fy} no {ext.upper()} (status {r.status_code})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {fy} {ext.upper()}: {e}\")\n",
    "    if not ok:\n",
    "        print(f\"[WARN] {fy} ({cid}) no usable file\")\n",
    "\n",
    "print(f\"\\nDone. Downloaded {downloaded}/{len(targets)} files into {OUT.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">b. Clean Energy Regulator (CER)</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committed shape: (25, 5)\n",
      "Probable  shape: (49, 4)\n",
      "Saved: /Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA2_LRET/committed_power_stations.csv\n",
      "Saved: /Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA2_LRET/probable_power_stations.csv\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# URL = \"https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data\"\n",
    "\n",
    "# # Output folder\n",
    "# OUT_DIR = Path(\"/Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA2_LRET\")\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # -------------------------\n",
    "# # Selenium setup (Headless)\n",
    "# # -------------------------\n",
    "# def make_driver():\n",
    "#     from selenium.webdriver.chrome.options import Options\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless=new\")\n",
    "#     options.add_argument(\"--no-sandbox\")\n",
    "#     options.add_argument(\"--disable-gpu\")\n",
    "#     options.add_argument(\"--window-size=1600,2400\")\n",
    "#     options.add_argument(\"--disable-dev-shm-usage\")\n",
    "#     options.add_argument(\"--lang=en-US\")\n",
    "#     options.add_argument(\n",
    "#         \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "#         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "#     )\n",
    "#     service = Service(ChromeDriverManager().install())\n",
    "#     return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# # -------------------------\n",
    "# # Helpers\n",
    "# # -------------------------\n",
    "# def norm(s: str) -> str:\n",
    "#     if s is None:\n",
    "#         return \"\"\n",
    "#     s = s.replace(\"\\xa0\", \" \")\n",
    "#     return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# def scroll_into_view(driver, el):\n",
    "#     try:\n",
    "#         driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", el)\n",
    "#         time.sleep(0.2)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# def find_table_after_heading(driver, heading_text: str):\n",
    "#     xpath_heading = (\n",
    "#         \"//*[self::h1 or self::h2 or self::h3 or self::h4]\"\n",
    "#         f\"[contains(translate(normalize-space(.),\"\n",
    "#         \" 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'),\"\n",
    "#         f\" '{heading_text.lower()}')]\"\n",
    "#     )\n",
    "#     heading = WebDriverWait(driver, 25).until(\n",
    "#         EC.presence_of_element_located((By.XPATH, xpath_heading))\n",
    "#     )\n",
    "#     scroll_into_view(driver, heading)\n",
    "#     table = heading.find_element(By.XPATH, \"following::table[1]\")\n",
    "#     scroll_into_view(driver, table)\n",
    "#     return table\n",
    "\n",
    "# def extract_headers_from_table(table_el):\n",
    "#     ths = table_el.find_elements(By.CSS_SELECTOR, \"thead th\")\n",
    "#     headers = [norm(th.text) for th in ths]\n",
    "#     if not headers:\n",
    "#         first_row_tds = table_el.find_elements(By.CSS_SELECTOR, \"tbody tr:first-child td\")\n",
    "#         if first_row_tds:\n",
    "#             headers = [f\"col_{i+1}\" for i in range(len(first_row_tds))]\n",
    "#     return headers\n",
    "\n",
    "# def try_set_length_near_table(table_el):\n",
    "#     candidates = []\n",
    "#     try:\n",
    "#         candidates += table_el.find_elements(By.XPATH, \"following::select[contains(@name,'length')][1]\")\n",
    "#         candidates += table_el.find_elements(By.XPATH, \"following::div[contains(@class,'length')]//select[1]\")\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     if not candidates:\n",
    "#         try:\n",
    "#             candidates = table_el.find_elements(By.XPATH, \"ancestor::div[1]//select\")\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     for sel in candidates:\n",
    "#         try:\n",
    "#             for want in [\"All\", \"100\", \"200\", \"250\", \"500\"]:\n",
    "#                 for opt in sel.find_elements(By.TAG_NAME, \"option\"):\n",
    "#                     if want.lower() in norm(opt.text).lower():\n",
    "#                         Select(sel).select_by_visible_text(opt.text)\n",
    "#                         time.sleep(1)\n",
    "#                         return True\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#     return False\n",
    "\n",
    "# def collect_all_rows_with_pagination(driver, table_el, max_pages=50):\n",
    "#     headers = extract_headers_from_table(table_el)\n",
    "\n",
    "#     def read_page_rows():\n",
    "#         rows = []\n",
    "#         for r in table_el.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "#             tds = r.find_elements(By.CSS_SELECTOR, \"td\")\n",
    "#             if not tds:\n",
    "#                 continue\n",
    "#             rows.append([norm(td.text) for td in tds])\n",
    "#         return rows\n",
    "\n",
    "#     # Step 1: try All/large page length\n",
    "#     changed = try_set_length_near_table(table_el)\n",
    "#     rows = read_page_rows()\n",
    "#     if changed:\n",
    "#         return headers, rows\n",
    "\n",
    "#     # Step 2: paginate with Next\n",
    "#     all_rows = []\n",
    "#     all_rows.extend(rows)\n",
    "\n",
    "#     def first_cell_text():\n",
    "#         try:\n",
    "#             return table_el.find_element(By.CSS_SELECTOR, \"tbody tr td\").text\n",
    "#         except Exception:\n",
    "#             return \"\"\n",
    "\n",
    "#     for _ in range(max_pages):\n",
    "#         before = norm(first_cell_text())\n",
    "#         next_btn = None\n",
    "#         for xpath in [\n",
    "#             \".//following::a[contains(.,'Next')][1]\",\n",
    "#             \".//following::button[contains(.,'Next')][1]\",\n",
    "#             \".//following::li[contains(@class,'next')]/a[1]\",\n",
    "#             \".//following::a[@aria-label='Next'][1]\",\n",
    "#         ]:\n",
    "#             try:\n",
    "#                 candidate = table_el.find_element(By.XPATH, xpath)\n",
    "#                 if candidate.is_displayed():\n",
    "#                     next_btn = candidate\n",
    "#                     break\n",
    "#             except Exception:\n",
    "#                 continue\n",
    "#         if not next_btn:\n",
    "#             break\n",
    "#         if \"disabled\" in (next_btn.get_attribute(\"class\") or \"\").lower():\n",
    "#             break\n",
    "\n",
    "#         scroll_into_view(driver, next_btn)\n",
    "#         try:\n",
    "#             ActionChains(driver).move_to_element(next_btn).click().perform()\n",
    "#         except:\n",
    "#             next_btn.click()\n",
    "\n",
    "#         try:\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 lambda d: norm(first_cell_text()) != before\n",
    "#             )\n",
    "#         except TimeoutException:\n",
    "#             break\n",
    "\n",
    "#         all_rows.extend(read_page_rows())\n",
    "\n",
    "#     return headers, all_rows\n",
    "\n",
    "# def to_dataframe(headers, rows):\n",
    "#     max_cols = max(len(headers), max((len(r) for r in rows), default=0))\n",
    "#     if len(headers) < max_cols:\n",
    "#         headers = headers + [f\"col_{i}\" for i in range(len(headers) + 1, max_cols + 1)]\n",
    "#     rows = [r + [\"\"] * (max_cols - len(r)) for r in rows]\n",
    "#     df = pd.DataFrame(rows, columns=headers)\n",
    "#     return df.dropna(how=\"all\")\n",
    "\n",
    "# def smart_rename(df):\n",
    "#     mapping = {}\n",
    "#     for c in df.columns:\n",
    "#         cl = norm(c).lower()\n",
    "#         if \"project\" in cl and \"name\" in cl:\n",
    "#             mapping[c] = \"Project Name\"\n",
    "#         elif cl == \"state\" or \"state\" in cl:\n",
    "#             mapping[c] = \"State\"\n",
    "#         elif (\"mw\" in cl and \"capacity\" in cl) or cl == \"mw\":\n",
    "#             mapping[c] = \"MW Capacity\"\n",
    "#         elif \"fuel\" in cl:\n",
    "#             mapping[c] = \"Fuel Source\"\n",
    "#         elif \"committed\" in cl and (\"date\" in cl or \"month\" in cl or \"year\" in cl):\n",
    "#             mapping[c] = \"Committed Date (Month/Year)\"\n",
    "#     return df.rename(columns=mapping)\n",
    "\n",
    "# def scrape_tables():\n",
    "#     driver = make_driver()\n",
    "#     try:\n",
    "#         driver.get(URL)\n",
    "#         WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "#         time.sleep(1)\n",
    "\n",
    "#         # Committed\n",
    "#         committed_table = find_table_after_heading(driver, \"Committed power stations\")\n",
    "#         headers_c, rows_c = collect_all_rows_with_pagination(driver, committed_table)\n",
    "#         df_committed = to_dataframe(headers_c, rows_c)\n",
    "#         df_committed = smart_rename(df_committed)\n",
    "\n",
    "#         # Probable\n",
    "#         probable_table = find_table_after_heading(driver, \"Probable power stations\")\n",
    "#         headers_p, rows_p = collect_all_rows_with_pagination(driver, probable_table)\n",
    "#         df_probable = to_dataframe(headers_p, rows_p)\n",
    "#         df_probable = smart_rename(df_probable)\n",
    "\n",
    "#         return df_committed, df_probable\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# def main():\n",
    "#     df_committed, df_probable = scrape_tables()\n",
    "\n",
    "#     print(\"Committed shape:\", df_committed.shape)\n",
    "#     print(\"Probable  shape:\", df_probable.shape)\n",
    "\n",
    "#     # Save only CSVs in target folder\n",
    "#     committed_path = OUT_DIR / \"committed_power_stations.csv\"\n",
    "#     probable_path = OUT_DIR / \"probable_power_stations.csv\"\n",
    "\n",
    "#     df_committed.to_csv(committed_path, index=False)\n",
    "#     df_probable.to_csv(probable_path, index=False)\n",
    "\n",
    "#     print(\"Saved:\", committed_path)\n",
    "#     print(\"Saved:\", probable_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to scrape the data\n",
    "def scrape_CER_data(table_id: str):\n",
    "    url = \"https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data\"\n",
    "\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=2000,1200\")\n",
    "    opts.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    wait = WebDriverWait(driver, 25)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the table is rendered\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"#{table_id}\")))\n",
    "        \n",
    "        # Will be faster if we show max rows first\n",
    "        length_sel = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"select[name='{table_id}_length']\")))\n",
    "        Select(length_sel).select_by_visible_text(\"100\")\n",
    "        \n",
    "        # wait for redraw\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, \".dataTables_processing\")))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Extract headers and rows\n",
    "        headers = [th.get_attribute(\"textContent\").strip() for th in driver.find_elements(By.CSS_SELECTOR, f\"#{table_id} thead th\")]\n",
    "        rows = []\n",
    "\n",
    "        # Handle pagination if max rows does not show all data\n",
    "        while True:\n",
    "            for tr in driver.find_elements(By.CSS_SELECTOR, f\"#{table_id} tbody tr\"):\n",
    "                tds = [td.get_attribute(\"textContent\").strip() for td in tr.find_elements(By.CSS_SELECTOR, \"td\")]\n",
    "                if tds:\n",
    "                    rows.append(tds)\n",
    "\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, f\"#{table_id}_wrapper button[data-dt-idx='next']\")\n",
    "\n",
    "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                break\n",
    "            \n",
    "            # Scroll to the next button first to ensure that it's clickable\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  \n",
    "            \n",
    "            # Click the button\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(2)\n",
    "\n",
    "        return pd.DataFrame(rows, columns=headers if headers else None)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to save the scraped data\n",
    "def save_CER_table(table_id: str, out_csv_path: str):\n",
    "\tout_path = Path(out_csv_path)\n",
    "\tout_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tdf = scrape_CER_data(table_id)\n",
    "\tdf.to_csv(out_path, index=False)\n",
    "\tprint(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA2\\approved_power_stations.csv\n",
      "Saved: DATA2\\committed_power_stations.csv\n",
      "Saved: DATA2\\probable_power_stations.csv\n"
     ]
    }
   ],
   "source": [
    "save_CER_table(\"DataTables_Table_0\", \"DATA2/approved_power_stations.csv\")\n",
    "save_CER_table(\"DataTables_Table_1\", \"DATA2/committed_power_stations.csv\")\n",
    "save_CER_table(\"DataTables_Table_2\", \"DATA2/probable_power_stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">c. Australian Bureau of Statistics (ABS)</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Population_and_people.xlsx ...\n",
      "Saved to /Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA3_ABS/Population_and_people.xlsx\n",
      "Downloading Economy_and_industry.xlsx ...\n",
      "Saved to /Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA3_ABS/Economy_and_industry.xlsx\n",
      "All files downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Target folder\n",
    "OUT_DIR = Path(\"/Users/ekaudiyani/Documents/KULIAH/SEMESTER 2/2. Data Engineering/Assignment_1/DATA3_ABS\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ABS file links\n",
    "files = {\n",
    "    \"Population_and_people.xlsx\": \"https://www.abs.gov.au/methodologies/data-region-methodology/2011-24/14100DO0001_2011-24.xlsx\",\n",
    "    \"Economy_and_industry.xlsx\": \"https://www.abs.gov.au/methodologies/data-region-methodology/2011-24/14100DO0003_2011-24.xlsx\",\n",
    "}\n",
    "\n",
    "# Download loop\n",
    "for fname, url in files.items():\n",
    "    out_path = OUT_DIR / fname\n",
    "    print(f\"Downloading {fname} ...\")\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()  # stop if error\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved to {out_path}\")\n",
    "\n",
    "print(\"All files downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">2. Data Cleaning & Integration</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "Combine the retrieved data into a single, consolidated database.  \n",
    "During this process, you may need to clean and pre-process the data to ensure consistency and reliability.  \n",
    "Tasks may include handling missing values, converting data types, and filtering out irrelevant or inconsistent data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">2.1 Data Cleaning</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> <span style=\"color:white\">a. NGER Data</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> <span style=\"color:white\">b. CER Data</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> <span style=\"color:white\">c. ABS Data</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">2.2 Data Integration</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">3. Data Augmentation</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "Augment your integrated dataset about large-scale power stations with their geo-location by programmatically querying the geographic coordinates  \n",
    "using a public geocoding API (such as Google Maps or OpenStreetMap/Nominatim) for all the energy facilities present.  \n",
    "Document methods and API usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">4. Data Transformation and Storage</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the processed and augmented data into a structured format suitable for analysis and visualization.  \n",
    "Specifically, you should:  \n",
    "• design a suitable database schema for storage in database, and  \n",
    "• implement this schema and store your data in either DuckDB or a PostgreSQL database.\n",
    "\n",
    "Whichever system you choose to install, make sure you include the spatial extensions so that we can run some spatial queries in Assignment 2.  \n",
    "This should be straight-forward for DuckDB, but when choosing PostgreSQL,  \n",
    "make sure PostGIS is included in the chosen install package.\n",
    "\n",
    "Important Note: Clearly justify your database design decisions (e.g., normalized or deformalized schema) in your project report.  \n",
    "If your group encounters significant difficulties working with a database, you may alternatively store your data in separate CSV files;  \n",
    "however, choosing CSV storage will result in a mark penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "CER_URL = \"https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data\"\n",
    "\n",
    "\n",
    "def _make_driver():\n",
    "\tfrom selenium.webdriver.chrome.options import Options\n",
    "\topts = Options()\n",
    "\topts.add_argument(\"--headless=new\")\n",
    "\topts.add_argument(\"--no-sandbox\")\n",
    "\topts.add_argument(\"--disable-gpu\")\n",
    "\topts.add_argument(\"--window-size=1600,1200\")\n",
    "\topts.add_argument(\"--disable-dev-shm-usage\")\n",
    "\topts.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\t# Selenium Manager will fetch a matching driver automatically (Selenium >= 4.6)\n",
    "\treturn webdriver.Chrome(options=opts)\n",
    "\n",
    "\n",
    "def scrap_CER_data(table_id: str) -> pd.DataFrame:\n",
    "\t\"\"\"Scrape a CER DataTables table by its DOM id (e.g., 'DataTables_Table_1').\"\"\"\n",
    "\tdriver = _make_driver()\n",
    "\twait = WebDriverWait(driver, 25)\n",
    "\ttry:\n",
    "\t\tdriver.get(CER_URL)\n",
    "\t\t# Wait for the table\n",
    "\t\ttable = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"#{table_id}\")))\n",
    "\t\t# Try to switch to 'All' if available; else fall back to largest option\n",
    "\t\ttry:\n",
    "\t\t\tlength_sel = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"select[name='{table_id}_length']\")))\n",
    "\t\t\ttry:\n",
    "\t\t\t\tSelect(length_sel).select_by_visible_text(\"All\")\n",
    "\t\t\texcept Exception:\n",
    "\t\t\t\t# choose the largest numeric option\n",
    "\t\t\t\topts = [(o.text.strip(), o) for o in length_sel.find_elements(By.TAG_NAME, \"option\")]\n",
    "\t\t\t\tnums = [(int(t), o) for t, o in opts if t.isdigit()]\n",
    "\t\t\t\tif nums:\n",
    "\t\t\t\t\tSelect(length_sel).select_by_visible_text(str(max(nums)[0]))\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\n",
    "\t\t# Headers\n",
    "\t\theaders = [th.text.strip() for th in table.find_elements(By.CSS_SELECTOR, \"thead th\")] or None\n",
    "\t\t# Rows\n",
    "\t\trows = []\n",
    "\t\tfor tr in table.find_elements(By.CSS_SELECTOR, \"tbody tr\"):\n",
    "\t\t\ttds = [td.text.strip() for td in tr.find_elements(By.TAG_NAME, \"td\")]\n",
    "\t\t\tif tds:\n",
    "\t\t\t\trows.append(tds)\n",
    "\t\treturn pd.DataFrame(rows, columns=headers)\n",
    "\tfinally:\n",
    "\t\tdriver.quit()\n",
    "\n",
    "\n",
    "def save_cer_table(table_id: str, out_csv_path: str) -> pd.DataFrame:\n",
    "\t\"\"\"Scrape by table id and save to CSV. Returns the DataFrame.\"\"\"\n",
    "\tout_path = Path(out_csv_path)\n",
    "\tout_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tdf = scrap_CER_data(table_id)\n",
    "\tdf.to_csv(out_path, index=False)\n",
    "\tprint(f\"Saved: {out_path}\")\n",
    "\treturn df\n",
    "\n",
    "# Save the requested tables\n",
    "_ = save_cer_table(\"DataTables_Table_1\", \"DATA2/committed_power_stations.csv\")\n",
    "_ = save_cer_table(\"DataTables_Table_2\", \"DATA2/probable_power_stations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
